- the idea is basically the same as with PDES1, that we try to provide a lookahead.
- But instead of doing it on road level, we do it on zone levels.


- each road needs to be assigned to a zone (based on its location => two passes of road files needed)
- a link can only belong to one zone at a time

- At the beginning: Initialize Zones.
     - This means: All roads which (potentially could) flow into different zones, must periodically schedule self/null messages.
     - This enables each zone to start
     
- The message queue of a zone can only be processed, if
              - a message has arrived from all incoming links to the zone
              - and only those messages can be processed, with a time stamp smaller than the time stamp of the smallest message
                from any incoming link
                
                
- a zone can be assigned to any executor thread. This means there can be more zones than processes. Any zones, which can progress
is just processed. Making more zones than threads/processors probably also leads to less waiting on locks? But on the other hand
it leads to more border links (really?).

- the look ahead increases in a certain way with many zones: normally there is a small look ahead for a big region (containing lots of potential events).
  But with smaller zones, the proportion between lookahead and zones size becomes smaller, so that ???? Is this true?
  
- We can make lots of message queues (one for each zone). Each MessageExcutor can then just do TryLocks and if not successful,
  just continue with the next zone.
  
- Simple divisions of zones at the beginning: Make them vertical. For example find out the x min and x max and then
  make equi-distant zones e.g. 10. and then get started the MessageExecutors to process them.
  
 => find out, if y direction is better than x direction.

- This approach will allow to progress empty zones quickly

- There will always be messages in the system, because of the null messages. So we have to set a maximum time
  for the simulation. As soon as the simulation is empty of real messages, the simulation will get ahead much
  faster anyway.
 
- THINK ABOUT IT:
    - Do we really need the new infrastructure of PDES1 with the EnterRequestMessage etc. Because we could also do it without or not?
    by directly 
    
    - does the inner road have an effect into the same junction, so that it could be processed to late or early? think about
    that also when doing implementation, that we get that also right...
    - 

Splitting the map:
 - (done) splitting the map in parts of same size can be very bad, if all most events happen in
   the same area.
 - (done) we could do the following: Make 1000 equidistant buckets of the map (e.g. x direction). Then look at the plan
   and find out, how many events will happen in which bucket. And then try to make only numberOfExecutorThread buckets
   from the 1000 equidistant buckets, by having the same number of events in each bucket.
   => This process can be accellarated by choosing random persons and their plans and put those events into the buckets.
   




- Problems:
    - it is possible, that in ZoneMessageQueue.getNextMessage the message queue is empty, even though
      the condition, that there are enough messages to process the queue condition is fullfilled
       => this leads to a null pointer exception there...
    - es gibt zu viele messages (ca. 3 mal mehr als anwendungs messages). Kann man das irgendwie 
      runterbringen?
      => vorallem das scheduling ist ein problem, weil es lockings braucht
      	- lösung 1: don't send any message to a node, if we just sent him a real message (sehr kleiner gain)
      	- lösung 2: try building bigger look aheads
        - lösung 3: (implemented): don't schedule the ZoneBorderMessages (because they actually do nothing) => implemented.
        - lösung 4: (implemented): make numberOfQueuedMessages more concurrent
            => more possible... 
    - main problem: incrementNumberOfQueuedMessages => concurrentHashMap mit ConcurrentMap.replace benutzt.
    - bad idea: don't send any border message, if there is a car on the road, but only set a timer
    	=> the getNextMessage would not function anymore...
    - next idea: warn a outBorder road, that traffic is coming. only registered roads should be informed
    	-> need to find a balance: too large areas for look ahead are fragil, too small provide small look ahead
    	=> start with look ahead of one or two roads
    	=> try resetting look ahead, when roads are empty again
    	 
    - wir haben zu viele timer messages zum schedulen
    
- TODO: 
     - performance verbessern
        - putMessage should be possible quickly (use my cuncurrent list implementation) => auch möglich mit nur 
        - wieso dauert getNextMessage so lang (obwohl empty buffers fast überhaupt nicht dauert).
        - getThreadId in MessageExecutor (ThreadLocal), dauert ca. 5.8sec zum ausführen
            => man könnte es der message mitgeben, etc...
        - empty buffer takes longer, than one would think...   
        - unix xwindow aus probieren
        - auch in normaler DES bzw. erster PDES version das Priority queue problem lösen
        - => grosse tests ausprobieren
        
        NEXT: 
        - implemented: such out border roads, which have no timer scheduled (because no car drives over them)
           => do static look ahead at the beginning and schedule all timers. In between do normal timers.
              - for such outborder messages, which will have no car on them during the whole simulation, they
              should send border message for time double.max and then do nothing.
           => resolve problem: perhaps too few events (test 8)
           => think again about the whole procedure, if everything is alright
             - hatte pracktisch einen deadlock auf satwal (wie konnte das passieren?)
           - perhaps use treeSet instead of PriorityQueue, because of the missing implementation of remove  
           - try out bigger input files (e.g. 10% planes).
           - should I make messagesArrivedFromRoads in ZoneMessageQueue volatile?
           
           - test, if logic of multi processor variant is the same as single cpu
        
           - problem: single cpu variante produziert zu wenig events bei test1 (test12 ok)
           
           - make tests and also consider the event count to be part of it
           
           - ich muss rausfinden, ob ein speed up stattfindet
           
           - es gib zu wenig events... bei einigen simulationen...
              => count different events to find out, which messages are missing...
           
           - es gibt ein exception beim iterieren, wie unterdrückt man dieses?
           
      - vorallem für grosse simulationen mit vielen agenten lohnt sich unsere simulation wahrscheinlich eher, weil dann
      sind viele agenten im gleichen area und die bordermessages werden nicht geschickt, weil diese durch normale messages
      ersetzt werden. => try input 7M
      
      
        
- Done:
     -  perhaps move arround the zone separators to achieve better lookahead propoerties? 
     -  perhaps doch several vertical Splits machen: dann können teile der Karte weiterarbeiten,
             anstatt, dass man von der anderen Seite abhängig ist.
             => Gegenargument: Man muss auf die andere Seite sowieso warten.
     - problem lösung: timer message muss höhere priorität bekommen als border message (mit gleichem time stamp),
      sonst bleibt message queue einfach stehen, weil keine neuen messages mehr produziert werden
     - MessageFactory updated to support full concurreny for each zone   
     - zone sizing is not equal yet => now each link is considered as a wait for each zone
     - getSimtime consumes around 15%+ of the CPU time => solved by removing ThreadLocal variable simTime in MessageExecutor
     - message counter removed, because it is a common synchronization point
   
     
     
     ===================================================
     - (solved) leave road, getFirst problem occured on satawal...
       => perhaps I am scheduling the message in the wrong messageQueue, because the original deadlock prevention message
          perhaps is already applied in the previous scheduler?
       => do we have a race condition between Deadlock prevention message and enterRequestMessage ? 
 		=> Fact: two times leave is done on a road (a Deadlock prevention message, which should have been timed out, was still alive)
 		=> the problem is not my problem, but rather that of java api: http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6207984
 		   - use queue.removeAll(Collections.singletonList(o)) instead of queue.remove for priority queues.
     =======================================================
     - when trying to replace lock by a hashmap class, there is some problem, that a message is missing for 
      some reason (not found out yet).
      - the problem was, that the read and write operation for messagesArrivedFromRoads needs to be outside  
     =======================================================
     - urgent: jetzt passiert heap space problem auch mit test 12 => das ist ein sehr kleiner test,
             d.h. sollte das eigentlich nicht passieren. => search the leak... => gelöst
     ==========================================================
     - find out, why there are less events using the new lookahead approach
           - wieso hat es soviele Messages (viel mehr als events), weil das sollte nicht mehr so sein.
           => es scheint, dass der split nicht perfekt ist... => zuviele events in einem bereich
     ==============================
       - man bleibt stecken bei 13 CPU version auf satwal => im debug mode nicht,
           aber im normalen mode schon????
              => 13 cpu => 2 zones hatten ein message zu wenig erhalten und konnten deshalb nicht weiterfahren
              => es gibt 1 message zu wenig be den zonen aus irgend einen grund
              => Lösung: es hat mit dem remove der message zu tun ( m!= null) in getNextMessage. Das problem muss behoben werden.
     =================================        
      - es kommen zu wenig events
           => max simulation time war zu niedrig eingestellt
     =====================================       
     
     
     
     
     
     
     
     
     
     
     
     
Experiments and Results
=========================
Ex1:
----
300K agents (arround 21M Events)
DES2, 1 CPU:  210 sec. (13.5 M events)
DES2, 6 CPU:  378 sec. (15 Millionen events)
DES2, 12 CPU: 210 sec. (12.7 M events)

Ex2:
----    
300K agents (arround 21M Events)
MobSim: 36min (events: wahrschein 21M)
DES2, 12 CPU:      
DES, 1 CPU: 

Ex3:
----
67K agents (arround )
PDES2, 1 CPU: 113.8 sec (7.66M events)
PDES2, 6 CPU: 38.8 sec (7.41M events)
PDES2, 12 CPU: 29.3 sec (7.14M events)
PDES, 1 CPU: 
MobSim: 
     